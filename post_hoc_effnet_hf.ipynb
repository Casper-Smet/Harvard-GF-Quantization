{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Post Hoc Quantisation of RNLFT Models with HuggingFace"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:55:49.097496Z",
     "start_time": "2024-06-24T00:55:47.511583Z"
    }
   },
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.optim import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from src.modules import *\n",
    "from src.data_handler import *\n",
    "from src import logger\n",
    "from src.class_balanced_loss import *\n",
    "from typing import NamedTuple\n",
    "from torchvision.models import efficientnet as efn\n",
    "\n",
    "from train_glaucoma_fair_fin_hf import train, validation, Identity_Info, quantifiable_efficientnet\n",
    "\n",
    "from fairlearn.metrics import *\n",
    "\n",
    "imb_info = Identity_Info()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:55:49.122898Z",
     "start_time": "2024-06-24T00:55:49.100010Z"
    }
   },
   "source": [
    "out_dim = 1\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "predictor_head = nn.Sigmoid()\n",
    "in_feat_to_final = 1280\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "fin_mu = 0.01\n",
    "fin_sigma = 1.\n",
    "fin_momentum = 0.3\n",
    "modality_types = 'rnflt'\n",
    "task = 'cls'\n",
    "model_type = 'efficientnet'  # 'resnext' or 'quant' or 'efficientnet'\n",
    "normalise_data = False\n",
    "if model_type == 'resnext':\n",
    "    if normalise_data:\n",
    "        pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_resnext_rnflt_Taskcls_lr5e-5_bz6_normdata1_2468_auc0.8516/best_weights.pth'\n",
    "    else:\n",
    "        pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_resnext_rnflt_Taskcls_lr5e-5_bz6_865_auc0.8510/last_weights.pth'\n",
    "elif model_type == 'efficientnet':\n",
    "    pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_efficientnet_rnflt_Taskcls_lr5e-5_bz6_normdata0_9764_auc0.8553/best_weights.pth'\n",
    "else:\n",
    "    if normalise_data:\n",
    "        pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_quant_rnflt_Taskcls_lr5e-5_bz6_normdata1_21_auc0.8450/best_weights.pth'\n",
    "    else:\n",
    "        pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_quant_rnflt_Taskcls_lr5e-5_bz6_9354_auc0.8495/best_weights.pth'\n",
    "ag_norm = Fair_Identity_Normalizer(\n",
    "    3,\n",
    "    dim=in_feat_to_final,\n",
    "    mu=fin_mu,\n",
    "    sigma=fin_sigma,\n",
    "    momentum=fin_momentum,\n",
    ")\n",
    "in_dim = 1\n",
    "# model = quantifiable_efficientnet(width_mult=1.0, depth_mult=1.0, weights=EfficientNet_B1_Weights.IMAGENET1K_V2)# create_model(model_type=model_type, in_dim=in_dim, out_dim=out_dim, include_final=False)\n",
    "# model = create_model(model_type=model_type, in_dim=in_dim, out_dim=out_dim, include_final=False)\n",
    "# final_layer = nn.Linear(in_features=in_feat_to_final, out_features=out_dim, bias=False)\n",
    "# model = nn.Sequential(model, ag_norm, final_layer)\n",
    "# model = model.to(device)\n",
    "# \n",
    "# checkpoint = torch.load(pretrained_weights)\n",
    "# \n",
    "# start_epoch = checkpoint['epoch'] + 1\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:55:49.388687Z",
     "start_time": "2024-06-24T00:55:49.123923Z"
    }
   },
   "source": [
    "data_dir = \"../quant_notes/data_cmpr\"\n",
    "image_size = 200\n",
    "attribute_type = 'race' \n",
    "\n",
    "trn_dataset = EyeFair(\n",
    "    os.path.join(data_dir, \"train\"),\n",
    "    depth=3 if model_type == \"resnext\" else 1,\n",
    "    modality_type=modality_types,\n",
    "    task=task,\n",
    "    resolution=image_size,\n",
    "    attribute_type=attribute_type,\n",
    "    normalise_data=normalise_data\n",
    "    \n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: -31.9900, max: 2.2700\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:55:49.391267Z",
     "start_time": "2024-06-24T00:55:49.389311Z"
    }
   },
   "source": [
    "batch_size = 6\n",
    "validation_dataset_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:55:50.159176Z",
     "start_time": "2024-06-24T00:55:50.156336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, criterion, optimizer, data_loader, epoch, identity_Info=None, _device='cuda'):\n",
    "    res = validation(model, criterion, None, validation_dataset_loader, 10, identity_Info=imb_info, _device=_device)\n",
    "    return res[1]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T20:21:57.821680Z",
     "start_time": "2024-06-23T20:21:57.646680Z"
    }
   },
   "source": "test(model, criterion, None, validation_dataset_loader, 10, identity_Info=imb_info, _device=device)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of numpy.float32",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_dataset_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midentity_Info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimb_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_device\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m, in \u001B[0;36mtest\u001B[0;34m(model, criterion, optimizer, data_loader, epoch, identity_Info, _device)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtest\u001B[39m(model, criterion, optimizer, data_loader, epoch, identity_Info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, _device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mvalidation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_dataset_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midentity_Info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimb_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_device\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_device\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/dev/uu/hcml/groupassingment/Harvard-GF-Quantization/train_glaucoma_fair_fin.py:223\u001B[0m, in \u001B[0;36mvalidation\u001B[0;34m(model, criterion, optimizer, validation_dataset_loader, epoch, result_dir, identity_Info, _device)\u001B[0m\n\u001B[1;32m    220\u001B[0m gts_by_attr \u001B[38;5;241m=\u001B[39m [ [] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(identity_Info\u001B[38;5;241m.\u001B[39mno_of_attr) ]\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 223\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (\u001B[38;5;28minput\u001B[39m, target, attr) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(validation_dataset_loader):\n\u001B[1;32m    224\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    225\u001B[0m         target \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:316\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:173\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m--> 141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m    144\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:222\u001B[0m, in \u001B[0;36mcollate_numpy_array_fn\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np_str_obj_array_pattern\u001B[38;5;241m.\u001B[39msearch(elem\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mstr) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem\u001B[38;5;241m.\u001B[39mdtype))\n\u001B[0;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m collate([torch\u001B[38;5;241m.\u001B[39mas_tensor(b) \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n",
      "File \u001B[0;32m~/miniconda3/envs/hardvard_gf_clean/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:222\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np_str_obj_array_pattern\u001B[38;5;241m.\u001B[39msearch(elem\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mstr) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem\u001B[38;5;241m.\u001B[39mdtype))\n\u001B[0;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m collate([\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Could not infer dtype of numpy.float32"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:55:55.227297Z",
     "start_time": "2024-06-24T00:55:54.975147Z"
    }
   },
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSize (MB):\u001B[39m\u001B[38;5;124m'\u001B[39m, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mgetsize(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemp.p\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m1e6\u001B[39m)\n\u001B[1;32m      4\u001B[0m     os\u001B[38;5;241m.\u001B[39mremove(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtemp.p\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m print_size_of_model(\u001B[43mmodel\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T19:55:52.128456Z",
     "start_time": "2024-06-23T19:55:52.109243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from optimum.quanto import qint8, quantize\n",
    "\n",
    "quantize(model, weights=qint8)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T19:55:57.460916Z",
     "start_time": "2024-06-23T19:55:57.421251Z"
    }
   },
   "cell_type": "code",
   "source": "print_size_of_model(model)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 26.575981\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##########################"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T22:20:25.242773Z",
     "start_time": "2024-06-23T22:20:24.881709Z"
    }
   },
   "cell_type": "code",
   "source": "!pip freeze | grep accelerate",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate==0.31.0\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T23:06:22.639043Z",
     "start_time": "2024-06-23T23:06:22.636129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_full_model(model_path):\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    wrapper = EfficientNetWrapper.from_pretrained(model_path)\n",
    "    \n",
    "    in_feat_to_final = wrapper.backbone.config.hidden_size\n",
    "    out_dim = config.num_labels\n",
    "    \n",
    "    ag_norm = nn.BatchNorm1d(in_feat_to_final)  # Assuming this is what ag_norm is\n",
    "    final_layer = nn.Linear(in_features=in_feat_to_final, out_features=out_dim, bias=False)\n",
    "    \n",
    "    full_model = nn.Sequential(wrapper, ag_norm, final_layer)\n",
    "    return full_model"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:56:00.298329Z",
     "start_time": "2024-06-24T00:56:00.292428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, QuantoConfig\n",
    "# from optimum.quanto import qint8, quantize\n",
    "\n",
    "model_path = \"/home/platelminto/Documents/uu/dev/hcml/groupassingment/Harvard-GF-Quantization/results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_efficientnet_rnflt_Taskcls_lr5e-5_bz6_normdata0_5694_auc0.7924/best_model\"\n",
    "\n",
    "# processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# quantization_config = QuantoConfig(weights=\"int8\")\n",
    "# \n",
    "# quantized_model = EfficientNetWrapper.from_pretrained(\n",
    "#     model_path,\n",
    "#     device_map=\"cuda:0\",\n",
    "#     quantization_config=quantization_config\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:56:00.782082Z",
     "start_time": "2024-06-24T00:56:00.640929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "from src.modules import EfficientNetWrapper\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path, device='cuda')\n",
    "\n",
    "model = EfficientNetWrapper(config)\n",
    "\n",
    "state_dict = load_file(model_path + \"/model.safetensors\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "print(model.device)\n",
    "print(torch.cuda.is_available())\n",
    "model.to('cuda')\n",
    "print(model.device)\n",
    "\n",
    "print_size_of_model(model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "True\n",
      "cuda:0\n",
      "Size (MB): 26.515317\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:56:05.635605Z",
     "start_time": "2024-06-24T00:56:02.642470Z"
    }
   },
   "cell_type": "code",
   "source": "test(model, criterion, None, validation_dataset_loader, 10, identity_Info=imb_info, _device=device)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test <==== epcoh 10 loss: 0.5644 auc: 0.8953\n",
      "0-attr auc: 0.9090\n",
      "1-attr auc: 0.8935\n",
      "2-attr auc: 0.8816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7585714285714286"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:56:10.803518Z",
     "start_time": "2024-06-24T00:56:10.518714Z"
    }
   },
   "cell_type": "code",
   "source": "from optimum.quanto import quantize, qint8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:56:11.611339Z",
     "start_time": "2024-06-24T00:56:11.588278Z"
    }
   },
   "cell_type": "code",
   "source": "quantize(model, weights=qint8)",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:56:13.732398Z",
     "start_time": "2024-06-24T00:56:13.692427Z"
    }
   },
   "cell_type": "code",
   "source": "print_size_of_model(model)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 26.609645\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:56:32.642252Z",
     "start_time": "2024-06-24T00:56:15.992501Z"
    }
   },
   "cell_type": "code",
   "source": "test(model, criterion, None, validation_dataset_loader, 10, identity_Info=imb_info, _device=device)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test <==== epcoh 10 loss: 0.5641 auc: 0.8946\n",
      "0-attr auc: 0.9087\n",
      "1-attr auc: 0.8929\n",
      "2-attr auc: 0.8803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7604761904761905"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:57:10.119921Z",
     "start_time": "2024-06-24T00:57:10.118042Z"
    }
   },
   "cell_type": "code",
   "source": "from optimum.quanto import freeze",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:57:14.392633Z",
     "start_time": "2024-06-24T00:57:14.371592Z"
    }
   },
   "cell_type": "code",
   "source": "freeze(model)",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:57:20.531828Z",
     "start_time": "2024-06-24T00:57:20.496565Z"
    }
   },
   "cell_type": "code",
   "source": "print_size_of_model(model)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 7.554601\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T00:57:31.662879Z",
     "start_time": "2024-06-24T00:57:26.474951Z"
    }
   },
   "cell_type": "code",
   "source": "test(model, criterion, None, validation_dataset_loader, 10, identity_Info=imb_info, _device=device)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test <==== epcoh 10 loss: 0.5641 auc: 0.8946\n",
      "0-attr auc: 0.9087\n",
      "1-attr auc: 0.8929\n",
      "2-attr auc: 0.8803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7604761904761905"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T22:34:10.783043Z",
     "start_time": "2024-06-23T22:34:10.780853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Ti\n",
      "Torch version: 2.3.1\n",
      "Transformers version: 4.42.0.dev0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T22:28:27.951507Z",
     "start_time": "2024-06-23T22:28:26.956095Z"
    }
   },
   "cell_type": "code",
   "source": "!pip show bitsandbytes",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\r\n",
      "Version: 0.43.1\r\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\r\n",
      "Home-page: https://github.com/TimDettmers/bitsandbytes\r\n",
      "Author: Tim Dettmers\r\n",
      "Author-email: dettmers@cs.washington.edu\r\n",
      "License: MIT\r\n",
      "Location: /home/platelminto/miniconda3/envs/harvard_gf/lib/python3.10/site-packages\r\n",
      "Requires: numpy, torch\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T18:58:37.883524Z",
     "start_time": "2024-06-21T18:58:37.848017Z"
    }
   },
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch.ao.quantization\n",
    "qmodel = deepcopy(model).to('cpu')\n",
    "if model_type == 'resnext':\n",
    "    qmodel[0].fuse_model(is_qat=False)\n",
    "else:\n",
    "    qmodel[0] = torch.quantization.QuantWrapper(qmodel[0])\n",
    "qmodel[1].v = False\n",
    "# qmodel = torch.ao.quantization.fuse_modules(model, ['conv2', 'bn2'])\n",
    "qmodel[2] = torch.quantization.QuantWrapper(qmodel[2])\n",
    "qmodel.qconfig = torch.ao.quantization.default_per_channel_qconfig\n",
    "print(qmodel.qconfig)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T18:59:08.447105Z",
     "start_time": "2024-06-21T18:58:42.566408Z"
    }
   },
   "source": [
    "import torch.ao.quantization\n",
    "\n",
    "\n",
    "qmodel.eval().to(\"cpu\")\n",
    "qconf = torch.quantization.QConfig(\n",
    "    activation=torch.quantization.MovingAverageMinMaxObserver.with_args(\n",
    "        qscheme=torch.per_tensor_symmetric\n",
    "    ),\n",
    "    weight=torch.quantization.MovingAveragePerChannelMinMaxObserver.with_args(\n",
    "        qscheme=torch.per_channel_symmetric, dtype=torch.qint8\n",
    "    ),\n",
    ")  # torch.ao.quantization.default_per_channel_qconfig.weight)\n",
    "qmodel.qconfig = qconf  # torch.ao.quantization.default_per_channel_qconfig\n",
    "print(qmodel.qconfig)\n",
    "torch.ao.quantization.prepare(qmodel, inplace=True)\n",
    "\n",
    "# Calibrate here\n",
    "res = validation(\n",
    "    qmodel,\n",
    "    criterion,\n",
    "    None,\n",
    "    validation_dataset_loader,\n",
    "    10,\n",
    "    identity_Info=imb_info,\n",
    "    _device=\"cpu\",\n",
    ")\n",
    "qmodel[1].v = False\n",
    "# Convert here\n",
    "torch.ao.quantization.convert(qmodel, inplace=True)\n",
    "print_size_of_model(qmodel)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_symmetric){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.per_channel_symmetric, dtype=torch.qint8){})\n",
      "cpu\n",
      "test <==== epcoh 10 loss: 249.9945 auc: 0.8568\n",
      "0-attr auc: 0.8765\n",
      "1-attr auc: 0.8508\n",
      "2-attr auc: 0.8280\n",
      "Size (MB): 8.068754\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T18:59:13.476460Z",
     "start_time": "2024-06-21T18:59:13.463944Z"
    }
   },
   "source": [
    "with torch.no_grad():\n",
    "        for i, (x, target, attr) in enumerate(validation_dataset_loader):\n",
    "            x = x.to(device)\n",
    "            target = target.to(device)\n",
    "            attr = attr.to(device)\n",
    "            break\n",
    "\n",
    "x.shape, target, attr"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 200, 200]),\n",
       " tensor([1., 1., 1., 1., 1., 0.], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 2], device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T19:38:39.112241Z",
     "start_time": "2024-06-23T19:38:39.001465Z"
    }
   },
   "source": [
    "res = validation(qmodel, criterion, None, validation_dataset_loader, 10, identity_Info=imb_info, _device=torch.device('cpu'))\n",
    "# next(model.parameters()).is_cuda"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::silu.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::silu.out' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/build/aten/src/ATen/RegisterCPU.cpp:31419 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/build/aten/src/ATen/RegisterCUDA.cpp:44504 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/build/aten/src/ATen/RegisterFunctionalization_1.cpp:25813 [kernel]\nNamed: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5369 [kernel]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradHIP: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradVE: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradMTIA: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/TraceType_3.cpp:14745 [kernel]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mvalidation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mqmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_dataset_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midentity_Info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimb_info\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_device\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/dev/uu/hcml/groupassingment/Harvard-GF-Quantization/train_glaucoma_fair_fin.py:228\u001B[0m, in \u001B[0;36mvalidation\u001B[0;34m(model, criterion, optimizer, validation_dataset_loader, epoch, result_dir, identity_Info, _device)\u001B[0m\n\u001B[1;32m    225\u001B[0m target \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    226\u001B[0m attr \u001B[38;5;241m=\u001B[39m attr\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m--> 228\u001B[0m pred, feat \u001B[38;5;241m=\u001B[39m \u001B[43mforward_model_with_fin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m pred \u001B[38;5;241m=\u001B[39m pred\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    231\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(pred, target)\n",
      "File \u001B[0;32m~/Documents/dev/uu/hcml/groupassingment/Harvard-GF-Quantization/src/modules.py:240\u001B[0m, in \u001B[0;36mforward_model_with_fin\u001B[0;34m(model, data, attr)\u001B[0m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward_model_with_fin\u001B[39m(model, data, attr):\n\u001B[0;32m--> 240\u001B[0m     feat \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(model[\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFair_Identity_Normalizer\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mQuantizable_FIN\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    242\u001B[0m         nml_feat \u001B[38;5;241m=\u001B[39m model[\u001B[38;5;241m1\u001B[39m](feat)\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/ao/quantization/stubs.py:63\u001B[0m, in \u001B[0;36mQuantWrapper.forward\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[1;32m     62\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquant(X)\n\u001B[0;32m---> 63\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdequant(X)\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torchvision/models/efficientnet.py:343\u001B[0m, in \u001B[0;36mEfficientNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torchvision/models/efficientnet.py:333\u001B[0m, in \u001B[0;36mEfficientNet._forward_impl\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 333\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavgpool(x)\n\u001B[1;32m    336\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/activation.py:396\u001B[0m, in \u001B[0;36mSiLU.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 396\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msilu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/functional.py:2101\u001B[0m, in \u001B[0;36msilu\u001B[0;34m(input, inplace)\u001B[0m\n\u001B[1;32m   2099\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(silu, (\u001B[38;5;28minput\u001B[39m,), \u001B[38;5;28minput\u001B[39m, inplace\u001B[38;5;241m=\u001B[39minplace)\n\u001B[1;32m   2100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[0;32m-> 2101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msilu_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39msilu(\u001B[38;5;28minput\u001B[39m)\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: Could not run 'aten::silu.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::silu.out' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/build/aten/src/ATen/RegisterCPU.cpp:31419 [kernel]\nCUDA: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/build/aten/src/ATen/RegisterCUDA.cpp:44504 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/build/aten/src/ATen/RegisterFunctionalization_1.cpp:25813 [kernel]\nNamed: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5369 [kernel]\nAutogradOther: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradCPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradCUDA: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradHIP: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradXLA: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradMPS: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradIPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradXPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradHPU: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradVE: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradLazy: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradMTIA: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradPrivateUse1: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradPrivateUse2: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradPrivateUse3: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradMeta: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nAutogradNestedTensor: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/VariableType_3.cpp:18859 [autograd kernel]\nTracer: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/torch/csrc/autograd/generated/TraceType_3.cpp:14745 [kernel]\nAutocastCPU: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at /opt/conda/conda-bld/pytorch_1716905971132/work/aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T13:10:19.229175Z",
     "start_time": "2024-06-21T13:10:19.225797Z"
    }
   },
   "source": [
    "res[1]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5157142857142857"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T13:10:21.962788Z",
     "start_time": "2024-06-21T13:10:21.945262Z"
    }
   },
   "source": [
    "# model\n",
    "qmodel"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantWrapper(\n",
       "    (quant): Quantize(scale=tensor([0.0499]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (dequant): DeQuantize()\n",
       "    (module): EfficientNet(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.06287065893411636, zero_point=128, padding=(1, 1), bias=False)\n",
       "          (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): QuantizedHardswish()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=3.2027080059051514, zero_point=128, padding=(1, 1), groups=32, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), scale=0.094724640250206, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.04096138849854469, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.3900623917579651, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.6104688048362732, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=5.385725021362305, zero_point=128, padding=(1, 1), groups=16, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), scale=0.06438656896352768, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.025406671687960625, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.16660472750663757, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.3180736303329468, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=2.203348398208618, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=8.899662971496582, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=5.7767205238342285, zero_point=128, padding=(1, 1), groups=96, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(96, 4, kernel_size=(1, 1), stride=(1, 1), scale=0.10594812780618668, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(4, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.01336989551782608, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.8973188400268555, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=1.9599477052688599, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=7.279860973358154, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.8599396347999573, zero_point=128, padding=(1, 1), groups=144, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), scale=0.28448158502578735, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.01740354672074318, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.2892846167087555, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.535127580165863, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.417157530784607, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=3.8811445236206055, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.5840805172920227, zero_point=128, padding=(1, 1), groups=144, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), scale=0.12286277860403061, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.027987759560346603, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.6500758528709412, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.7042642831802368, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=2.830416202545166, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=13.564194679260254, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), scale=1.1291786432266235, zero_point=128, padding=(2, 2), groups=144, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), scale=0.2304016351699829, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.014090987853705883, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.3241167366504669, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.6278660297393799, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=6.321447372436523, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), scale=4.9006028175354, zero_point=128, padding=(2, 2), groups=240, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.07931049913167953, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.05702095106244087, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=6.34966516494751, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), scale=4.360027313232422, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=11.7587308883667, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=27.284181594848633, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), scale=4.399153232574463, zero_point=128, padding=(2, 2), groups=240, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.2460424154996872, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.07334578782320023, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=3.935295581817627, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), scale=6.398533344268799, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=14.245017051696777, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=44.99180221557617, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), scale=3.04390549659729, zero_point=128, padding=(1, 1), groups=240, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.1462179273366928, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.012471340596675873, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=10.758936882019043, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), scale=6.52214241027832, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=60.70524978637695, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=37.55424880981445, zero_point=128, padding=(1, 1), groups=480, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=1.15096914768219, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=8.167512893676758, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=101.82247924804688, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=126.0999526977539, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=703.8955078125, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=1108.1630859375, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=473.9874572753906, zero_point=128, padding=(1, 1), groups=480, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=81.86090087890625, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=131.10923767089844, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=397.91949462890625, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=266.2981872558594, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1772.6572265625, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=5322.89111328125, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=475.5365295410156, zero_point=128, padding=(1, 1), groups=480, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=19.630125045776367, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=35.20242691040039, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=1100.1981201171875, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=435.4877014160156, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=2602.893310546875, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=8416.0390625, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), scale=1437.2935791015625, zero_point=128, padding=(2, 2), groups=480, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=23.21344757080078, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=59.99717330932617, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=1187.542724609375, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), scale=716.13671875, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=9052.7587890625, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=2017.1868896484375, zero_point=128, padding=(2, 2), groups=672, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=31.708637237548828, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=19.66274070739746, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=889.9107055664062, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=666.685302734375, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=3449.265869140625, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=9199.015625, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=585.8705444335938, zero_point=128, padding=(2, 2), groups=672, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=56.27672576904297, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=38.240997314453125, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=2378.965087890625, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=1433.5394287109375, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=6819.65576171875, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=13780.333984375, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=1057.4913330078125, zero_point=128, padding=(2, 2), groups=672, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=69.33000946044922, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=125.20764923095703, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=943.4349365234375, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=550.9766845703125, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=6299.92919921875, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=20125.080078125, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), scale=1375.5882568359375, zero_point=128, padding=(2, 2), groups=672, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=9.150795936584473, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=1.8362303972244263, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=4839.3056640625, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), scale=10176.4150390625, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=52727.82421875, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=3446.684326171875, zero_point=128, padding=(2, 2), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=193.51101684570312, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=260.0831298828125, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=3111.689453125, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=1127.6011962890625, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=16571.66796875, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=52915.3984375, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=6192.57470703125, zero_point=128, padding=(2, 2), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=317.1035461425781, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=253.33274841308594, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=1170.050537109375, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=736.2938232421875, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=16621.912109375, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=57606.2890625, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=2746.974609375, zero_point=128, padding=(2, 2), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=384.2710876464844, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=384.5694885253906, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=956.6590576171875, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=371.2626647949219, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=16643.328125, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=66730.5, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=3522.1708984375, zero_point=128, padding=(2, 2), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=267.62353515625, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=297.8288269042969, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=2368.2373046875, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=1128.994873046875, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=17613.919921875, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=96865.8671875, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), scale=3456.0400390625, zero_point=128, padding=(1, 1), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=74.27178192138672, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=200.3469696044922, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=2913.143310546875, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), scale=11223.2470703125, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), scale=60402.0390625, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), scale=4153.22216796875, zero_point=128, padding=(1, 1), groups=1920, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1), scale=953.3563232421875, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1), scale=1481.3358154296875, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=1288.8740234375, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), scale=1152.8809814453125, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=27917.693359375, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Conv2dNormActivation(\n",
       "          (0): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=39102.25, zero_point=128, bias=False)\n",
       "          (1): QuantizedBatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): QuantizedHardswish()\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (classifier): Sequential(\n",
       "        (0): QuantizedDropout(p=0.2, inplace=True)\n",
       "        (1): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): G0: (0.000423, 0.803667), G1: (0.000218, 0.802622), G2: (0.000219, 0.818224)\n",
       "  (2): QuantWrapper(\n",
       "    (quant): Quantize(scale=tensor([264.7185]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (dequant): DeQuantize()\n",
       "    (module): QuantizedLinear(in_features=1280, out_features=1, scale=19.896682739257812, zero_point=128, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'qconfig'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mqmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mqconfig\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1708\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1709\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Sequential' object has no attribute 'qconfig'"
     ]
    }
   ],
   "source": [
    "qmodel.qconfig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harvard_gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
