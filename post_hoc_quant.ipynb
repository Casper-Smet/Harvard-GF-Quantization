{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Hoc Quantisation of RNLFT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.optim import *\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from src.modules import *\n",
    "from src.data_handler import *\n",
    "from src import logger\n",
    "from src.class_balanced_loss import *\n",
    "from typing import NamedTuple\n",
    "from torchvision.models import efficientnet as efn\n",
    "\n",
    "from train_glaucoma_fair_fin import train, validation, Identity_Info, quantifiable_efficientnet\n",
    "\n",
    "from fairlearn.metrics import *\n",
    "\n",
    "imb_info = Identity_Info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = 1\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "predictor_head = nn.Sigmoid()\n",
    "in_feat_to_final = 1280\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "fin_mu = 0.01\n",
    "fin_sigma = 1.\n",
    "fin_momentum = 0.3\n",
    "model_type = 'efficientnet'\n",
    "modality_types = 'rnflt'\n",
    "task = 'cls'\n",
    "pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_efficientnet_rnflt_Taskcls_lr5e-5_bz6_564_auc0.8569/last_weights.pth'\n",
    "pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_efficientnet_rnflt_Taskcls_lr5e-5_bz6_564_auc0.8569/best_weights.pth'\n",
    "pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_quant_rnflt_Taskcls_lr5e-5_bz6_4442_auc0.7311/best_weights.pth'\n",
    "pretrained_weights = 'results/crosssectional_rnflt_fin_race_ablation_of_sigma/fullysup_quant_rnflt_Taskcls_lr5e-5_bz6_9354_auc0.8495/best_weights.pth'\n",
    "ag_norm = Fair_Identity_Normalizer(\n",
    "    3,\n",
    "    dim=in_feat_to_final,\n",
    "    mu=fin_mu,\n",
    "    sigma=fin_sigma,\n",
    "    momentum=fin_momentum,\n",
    ")\n",
    "in_dim = 1\n",
    "# model = quantifiable_efficientnet(width_mult=1.0, depth_mult=1.0, weights=EfficientNet_B1_Weights.IMAGENET1K_V2)# create_model(model_type=model_type, in_dim=in_dim, out_dim=out_dim, include_final=False)\n",
    "model = quantifiable_efficientnet(width_mult=1.0, depth_mult=1.1)# create_model(model_type=model_type, in_dim=in_dim, out_dim=out_dim, include_final=False)\n",
    "final_layer = nn.Linear(in_features=in_feat_to_final, out_features=out_dim, bias=False)\n",
    "model = nn.Sequential(model, ag_norm, final_layer)\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint = torch.load(pretrained_weights)\n",
    "\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "efnm = create_model(model_type=model_type, in_dim=in_dim, out_dim=out_dim, include_final=False)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "# scheduler.load_state_dict(checkpoint['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: -31.9900, max: 2.2700\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../quant_notes/data_cmpr\"\n",
    "image_size = 200\n",
    "attribute_type = 'race' \n",
    "\n",
    "trn_dataset = EyeFair(\n",
    "    os.path.join(data_dir, \"train\"),\n",
    "    modality_type=modality_types,\n",
    "    task=task,\n",
    "    resolution=image_size,\n",
    "    attribute_type=attribute_type,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "validation_dataset_loader = torch.utils.data.DataLoader(trn_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "test <==== epcoh 10 loss: 1.6386 auc: 0.8474\n",
      "0-attr auc: 0.8725\n",
      "1-attr auc: 0.8414\n",
      "2-attr auc: 0.8169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7657142857142857"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = validation(model, criterion, None, validation_dataset_loader, 10, identity_Info=imb_info, _device=device)\n",
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 26.498741\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # self.quant = QuantStub()\n",
    "        # self.dequant = DeQuantStub()\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch.ao.quantization\n",
    "qmodel = deepcopy(model)\n",
    "qmodel[1].v = False\n",
    "# qmodel = torch.ao.quantization.fuse_modules(model, ['conv2', 'bn2'])\n",
    "qmodel[0] = torch.quantization.QuantWrapper(qmodel[0])\n",
    "qmodel[2] = torch.quantization.QuantWrapper(qmodel[2])\n",
    "# pre_model_stub = nn.Sequential(torch.ao.quantization.QuantStub(), qmodel[0])\n",
    "# post_model_stub =  nn.Sequential(qmodel[-1], torch.ao.quantization.DeQuantStub())\n",
    "# qmodel[0] = pre_model_stub\n",
    "# qmodel[-1] = post_model_stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_symmetric){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.per_channel_symmetric, dtype=torch.qint8){})\n",
      "cpu\n",
      "test <==== epcoh 10 loss: 1.6385 auc: 0.8474\n",
      "0-attr auc: 0.8725\n",
      "1-attr auc: 0.8414\n",
      "2-attr auc: 0.8169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pcsmet/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/ao/quantization/utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 8.143738\n"
     ]
    }
   ],
   "source": [
    "import torch.ao.quantization\n",
    "\n",
    "\n",
    "qmodel.eval().to(\"cpu\")\n",
    "qconf = torch.quantization.QConfig(\n",
    "    activation=torch.quantization.MovingAverageMinMaxObserver.with_args(\n",
    "        qscheme=torch.per_tensor_symmetric\n",
    "    ),\n",
    "    weight=torch.quantization.MovingAveragePerChannelMinMaxObserver.with_args(\n",
    "        qscheme=torch.per_channel_symmetric, dtype=torch.qint8\n",
    "    ),\n",
    ")  # torch.ao.quantization.default_per_channel_qconfig.weight)\n",
    "qmodel.qconfig = qconf  # torch.ao.quantization.default_per_channel_qconfig\n",
    "print(qmodel.qconfig)\n",
    "torch.ao.quantization.prepare(qmodel, inplace=True)\n",
    "\n",
    "# Calibrate here\n",
    "res = validation(\n",
    "    qmodel,\n",
    "    criterion,\n",
    "    None,\n",
    "    validation_dataset_loader,\n",
    "    10,\n",
    "    identity_Info=imb_info,\n",
    "    _device=\"cpu\",\n",
    ")\n",
    "qmodel[1].v = True\n",
    "# Convert here\n",
    "torch.ao.quantization.convert(qmodel, inplace=True)\n",
    "print_size_of_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        for i, (x, target, attr) in enumerate(validation_dataset_loader):\n",
    "            x = x.to(device)\n",
    "            target = target.to(device)\n",
    "            attr = attr.to(device)\n",
    "            break\n",
    "\n",
    "x.shape, target, attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 1.1041,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [1.1041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 3.3123, 1.1041,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 2.2082, 4.4165,  ..., 0.0000, 2.2082, 1.1041],\n",
      "        [1.1041, 1.1041, 1.1041,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.5206,  2.2082,  5.5206,  ...,  2.2082,  1.1041,  2.2082],\n",
      "        [ 3.3123,  5.5206, 11.0411,  ...,  1.1041,  1.1041,  5.5206],\n",
      "        [ 0.0000,  1.1041,  2.2082,  ...,  0.0000,  0.0000,  1.1041],\n",
      "        [ 1.1041,  1.1041,  6.6247,  ...,  4.4165,  3.3123,  2.2082]])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 4.4165,  ..., 0.0000, 2.2082, 2.2082],\n",
      "        [3.3123, 3.3123, 7.7288,  ..., 2.2082, 2.2082, 5.5206],\n",
      "        [1.1041, 1.1041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [2.2082, 2.2082, 1.1041,  ..., 0.0000, 0.0000, 1.1041]])\n",
      "tensor([[0.0000, 1.1041, 0.0000,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [6.6247, 4.4165, 5.5206,  ..., 5.5206, 0.0000, 3.3123],\n",
      "        [5.5206, 3.3123, 5.5206,  ..., 1.1041, 1.1041, 4.4165],\n",
      "        [1.1041, 2.2082, 4.4165,  ..., 1.1041, 3.3123, 3.3123],\n",
      "        [1.1041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[2.2082, 2.2082, 1.1041,  ..., 0.0000, 1.1041, 4.4165],\n",
      "        [0.0000, 1.1041, 1.1041,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 1.1041,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 3.3123,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 0.0000, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [2.2082, 2.2082, 1.1041,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[5.5206, 5.5206, 9.9370,  ..., 4.4165, 5.5206, 4.4165],\n",
      "        [0.0000, 0.0000, 4.4165,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 2.2082,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [4.4165, 5.5206, 4.4165,  ..., 2.2082, 5.5206, 2.2082],\n",
      "        [1.1041, 2.2082, 5.5206,  ..., 0.0000, 2.2082, 3.3123]])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 3.3123,  ..., 0.0000, 1.1041, 2.2082],\n",
      "        [0.0000, 0.0000, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 3.3123, 3.3123,  ..., 2.2082, 1.1041, 5.5206]])\n",
      "tensor([[2.2082, 1.1041, 1.1041,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [3.3123, 3.3123, 7.7288,  ..., 4.4165, 2.2082, 6.6247],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 1.1041, 1.1041,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [2.2082, 0.0000, 1.1041,  ..., 2.2082, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 4.4165,  ..., 0.0000, 2.2082, 2.2082]])\n",
      "tensor([[1.1041, 1.1041, 2.2082,  ..., 2.2082, 1.1041, 2.2082],\n",
      "        [1.1041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 0.0000, 1.1041,  ..., 0.0000, 2.2082, 1.1041],\n",
      "        [1.1041, 1.1041, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 1.1041,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[2.2082, 5.5206, 5.5206,  ..., 2.2082, 2.2082, 3.3123],\n",
      "        [1.1041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 1.1041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 2.2082,  ..., 0.0000, 0.0000, 1.1041],\n",
      "        [0.0000, 2.2082, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[2.2082, 1.1041, 1.1041,  ..., 2.2082, 0.0000, 1.1041],\n",
      "        [2.2082, 3.3123, 3.3123,  ..., 0.0000, 1.1041, 2.2082],\n",
      "        [1.1041, 0.0000, 3.3123,  ..., 0.0000, 0.0000, 1.1041],\n",
      "        [2.2082, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0000, 1.1041, 3.3123,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 3.3123, 2.2082,  ..., 0.0000, 2.2082, 1.1041],\n",
      "        [0.0000, 1.1041, 3.3123,  ..., 1.1041, 2.2082, 2.2082]])\n",
      "tensor([[2.2082, 2.2082, 5.5206,  ..., 0.0000, 1.1041, 3.3123],\n",
      "        [2.2082, 2.2082, 2.2082,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 5.5206,  ..., 0.0000, 1.1041, 2.2082],\n",
      "        [1.1041, 1.1041, 3.3123,  ..., 0.0000, 0.0000, 3.3123],\n",
      "        [0.0000, 0.0000, 2.2082,  ..., 1.1041, 0.0000, 1.1041],\n",
      "        [0.0000, 0.0000, 5.5206,  ..., 0.0000, 1.1041, 1.1041]])\n",
      "tensor([[2.2082, 4.4165, 7.7288,  ..., 6.6247, 4.4165, 7.7288],\n",
      "        [2.2082, 1.1041, 1.1041,  ..., 1.1041, 1.1041, 0.0000],\n",
      "        [1.1041, 5.5206, 7.7288,  ..., 2.2082, 3.3123, 5.5206],\n",
      "        [0.0000, 1.1041, 0.0000,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [1.1041, 3.3123, 3.3123,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [1.1041, 1.1041, 1.1041,  ..., 0.0000, 1.1041, 0.0000]])\n",
      "tensor([[1.1041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 1.1041, 0.0000,  ..., 1.1041, 1.1041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 0.0000, 1.1041,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [1.1041, 4.4165, 5.5206,  ..., 2.2082, 5.5206, 3.3123],\n",
      "        [2.2082, 1.1041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0000, 2.2082, 3.3123,  ..., 1.1041, 1.1041, 1.1041],\n",
      "        [0.0000, 1.1041, 4.4165,  ..., 1.1041, 1.1041, 2.2082],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 2.2082, 4.4165,  ..., 0.0000, 2.2082, 3.3123],\n",
      "        [1.1041, 1.1041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0000, 1.1041, 1.1041,  ..., 0.0000, 5.5206, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.1041, 0.0000, 1.1041],\n",
      "        [2.2082, 3.3123, 7.7288,  ..., 2.2082, 1.1041, 2.2082],\n",
      "        [0.0000, 1.1041, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [2.2082, 1.1041, 1.1041,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1041,  ..., 0.0000, 1.1041, 2.2082]])\n",
      "tensor([[2.2082, 2.2082, 1.1041,  ..., 1.1041, 0.0000, 1.1041],\n",
      "        [1.1041, 0.0000, 1.1041,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 1.1041, 2.2082,  ..., 0.0000, 1.1041, 3.3123]])\n",
      "tensor([[0.0000, 0.0000, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1041,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 1.1041, 0.0000,  ..., 0.0000, 0.0000, 2.2082],\n",
      "        [2.2082, 2.2082, 3.3123,  ..., 2.2082, 1.1041, 3.3123],\n",
      "        [0.0000, 1.1041, 4.4165,  ..., 1.1041, 0.0000, 1.1041]])\n",
      "tensor([[2.2082, 1.1041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [4.4165, 2.2082, 4.4165,  ..., 0.0000, 2.2082, 1.1041],\n",
      "        [0.0000, 1.1041, 1.1041,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 3.3123, 3.3123,  ..., 2.2082, 4.4165, 3.3123],\n",
      "        [1.1041, 1.1041, 2.2082,  ..., 0.0000, 1.1041, 7.7288],\n",
      "        [1.1041, 4.4165, 5.5206,  ..., 0.0000, 0.0000, 2.2082]])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 3.3123, 7.7288,  ..., 0.0000, 1.1041, 2.2082],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [0.0000, 2.2082, 2.2082,  ..., 0.0000, 2.2082, 2.2082],\n",
      "        [1.1041, 0.0000, 4.4165,  ..., 1.1041, 0.0000, 2.2082],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 0.0000, 1.1041,  ..., 0.0000, 0.0000, 1.1041],\n",
      "        [3.3123, 2.2082, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 1.1041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [2.2082, 2.2082, 3.3123,  ..., 1.1041, 1.1041, 1.1041],\n",
      "        [0.0000, 1.1041, 3.3123,  ..., 0.0000, 2.2082, 1.1041]])\n",
      "tensor([[1.1041, 1.1041, 1.1041,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 5.5206,  ..., 0.0000, 1.1041, 1.1041],\n",
      "        [1.1041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [3.3123, 2.2082, 1.1041,  ..., 1.1041, 2.2082, 0.0000],\n",
      "        [2.2082, 2.2082, 4.4165,  ..., 0.0000, 1.1041, 3.3123]])\n",
      "tensor([[1.1041, 0.0000, 0.0000,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 3.3123,  ..., 1.1041, 2.2082, 5.5206],\n",
      "        [0.0000, 0.0000, 1.1041,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [1.1041, 1.1041, 1.1041,  ..., 0.0000, 1.1041, 2.2082],\n",
      "        [1.1041, 1.1041, 1.1041,  ..., 1.1041, 0.0000, 0.0000],\n",
      "        [1.1041, 3.3123, 2.2082,  ..., 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[1.1041, 3.3123, 1.1041,  ..., 2.2082, 0.0000, 2.2082],\n",
      "        [1.1041, 2.2082, 1.1041,  ..., 0.0000, 1.1041, 0.0000],\n",
      "        [0.0000, 0.0000, 2.2082,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1041, 2.2082,  ..., 1.1041, 0.0000, 2.2082],\n",
      "        [0.0000, 1.1041, 2.2082,  ..., 0.0000, 1.1041, 3.3123],\n",
      "        [1.1041, 0.0000, 1.1041,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentity_Info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimb_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# next(model.parameters()).is_cuda\u001b[39;00m\n",
      "File \u001b[0;32m~/University/Period4/quant/train_glaucoma_fair_fin.py:221\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(model, criterion, optimizer, validation_dataset_loader, epoch, result_dir, identity_Info, _device)\u001b[0m\n\u001b[1;32m    218\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    219\u001b[0m attr \u001b[38;5;241m=\u001b[39m attr\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 221\u001b[0m pred, feat \u001b[38;5;241m=\u001b[39m \u001b[43mforward_model_with_fin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    224\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, target)\n",
      "File \u001b[0;32m~/University/Period4/quant/src/modules.py:239\u001b[0m, in \u001b[0;36mforward_model_with_fin\u001b[0;34m(model, data, attr)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_model_with_fin\u001b[39m(model, data, attr):\n\u001b[0;32m--> 239\u001b[0m     feat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(model[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFair_Identity_Normalizer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantizable_FIN\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    241\u001b[0m         nml_feat \u001b[38;5;241m=\u001b[39m model[\u001b[38;5;241m1\u001b[39m](feat)\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/ao/quantization/stubs.py:63\u001b[0m, in \u001b[0;36mQuantWrapper.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m     62\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(X)\n\u001b[0;32m---> 63\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torchvision/models/efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torchvision/models/efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    336\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/University/Period4/quant/src/quant_efficientnet.py:38\u001b[0m, in \u001b[0;36mQuantizableMBConv.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# return self.skip_mul.mul(self._scale(input), input)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m     40\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_add\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result))\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/batchnorm.py:72\u001b[0m, in \u001b[0;36mBatchNorm2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# disabling this since this is not symbolically traceable\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# self._check_input_dim(input)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mbatch_norm2d(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean,\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_point)\n",
      "File \u001b[0;32m~/miniconda3/envs/harvard_gf/lib/python3.10/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = validation(qmodel, criterion, None, validation_dataset_loader, 10, identity_Info=imb_info, _device=torch.device('cpu'))\n",
    "# next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5114285714285715"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantWrapper(\n",
       "    (quant): Quantize(scale=tensor([2.5769]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (dequant): DeQuantize()\n",
       "    (module): EfficientNet(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): QuantizedConv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), scale=3.3178462982177734, zero_point=128, padding=(1, 1), bias=False)\n",
       "          (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): QuantizedHardswish()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=3.739513635635376, zero_point=128, padding=(1, 1), groups=32, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), scale=0.1100742369890213, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.041001707315444946, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=1.0450955629348755, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=1.7479215860366821, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), scale=11.76651382446289, zero_point=128, padding=(1, 1), groups=16, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), scale=0.05762992426753044, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(4, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.02540614642202854, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.3314040005207062, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.5631916522979736, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=4.8798956871032715, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=18.995813369750977, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=12.838979721069336, zero_point=128, padding=(1, 1), groups=96, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(96, 4, kernel_size=(1, 1), stride=(1, 1), scale=0.104728564620018, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(4, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.013168814592063427, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=1.6377025842666626, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=4.732588768005371, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=13.314597129821777, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=1.5372827053070068, zero_point=128, padding=(1, 1), groups=144, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), scale=0.2813253104686737, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.017314814031124115, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.5171109437942505, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.9865963459014893, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=3.457597255706787, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=7.2211127281188965, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=1.1766566038131714, zero_point=128, padding=(1, 1), groups=144, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), scale=0.1174837052822113, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.024589376524090767, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.5331462025642395, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.712088406085968, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=3.494774341583252, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=15.573673248291016, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), scale=0.975229799747467, zero_point=128, padding=(2, 2), groups=144, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), scale=0.22815577685832977, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.013820827938616276, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.20236007869243622, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.6014249920845032, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=3.9571692943573, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), scale=2.4250271320343018, zero_point=128, padding=(2, 2), groups=240, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.06564963608980179, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.03410246968269348, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.40268465876579285, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.3854096233844757, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.3777295351028442, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=5.058089733123779, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), scale=1.3471180200576782, zero_point=128, padding=(2, 2), groups=240, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.06900041550397873, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.015981705859303474, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.19205090403556824, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), scale=0.22486837208271027, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.3206580877304077, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), scale=5.452017784118652, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), scale=0.3123414218425751, zero_point=128, padding=(1, 1), groups=240, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), scale=0.11073312163352966, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), scale=0.010446406900882721, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.20176388323307037, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.3746317923069, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=1.8862775564193726, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=0.31966540217399597, zero_point=128, padding=(1, 1), groups=480, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=0.06996964663267136, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.3296152949333191, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=0.28892895579338074, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.31528329849243164, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=2.068848133087158, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=3.659614324569702, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=0.7394441366195679, zero_point=128, padding=(1, 1), groups=480, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=0.10263417661190033, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.15274767577648163, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=1.2405164241790771, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=0.7908869385719299, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=4.717267990112305, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=16.411067962646484, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), scale=1.628774642944336, zero_point=128, padding=(1, 1), groups=480, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=0.06759205460548401, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.0910930186510086, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=4.129389762878418, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), scale=2.4161946773529053, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=8.019213676452637, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), scale=29.326534271240234, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), scale=5.935046195983887, zero_point=128, padding=(2, 2), groups=480, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), scale=0.10729760676622391, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), scale=0.22112013399600983, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=3.433102607727051, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), scale=2.460390567779541, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=29.226533889770508, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=6.131525039672852, zero_point=128, padding=(2, 2), groups=672, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=0.1401216983795166, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.1344449818134308, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=2.8874993324279785, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=2.148892879486084, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=18.48659324645996, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=34.1439208984375, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=2.183051586151123, zero_point=128, padding=(2, 2), groups=672, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=0.15880876779556274, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.2109583616256714, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=10.75595760345459, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=5.9972028732299805, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=32.98470687866211, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=77.34615325927734, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), scale=5.070506572723389, zero_point=128, padding=(2, 2), groups=672, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=0.5003522038459778, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.6252949237823486, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=5.991062164306641, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), scale=3.8181052207946777, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=29.24395179748535, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), scale=91.30677032470703, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), scale=6.278590202331543, zero_point=128, padding=(2, 2), groups=672, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), scale=0.1469583809375763, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), scale=0.012248530983924866, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=14.456639289855957, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), scale=30.364601135253906, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=155.8777313232422, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=11.435833930969238, zero_point=128, padding=(2, 2), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.6865474581718445, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.9053087830543518, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=8.028745651245117, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=3.0763721466064453, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=49.98648452758789, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (2): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=157.45140075683594, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=19.2143611907959, zero_point=128, padding=(2, 2), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=1.122694730758667, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.8657562732696533, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=7.010431289672852, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=2.6361844539642334, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=49.590232849121094, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (3): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=176.66162109375, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=8.66685962677002, zero_point=128, padding=(2, 2), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=1.2003726959228516, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=1.84127938747406, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=3.2726235389709473, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=1.9579964876174927, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=49.40989685058594, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (4): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=198.29103088378906, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), scale=10.960318565368652, zero_point=128, padding=(2, 2), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=1.1522294282913208, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=1.499837875366211, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=4.098392963409424, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), scale=2.20580792427063, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=49.448734283447266, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=277.3419494628906, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), scale=10.166165351867676, zero_point=128, padding=(1, 1), groups=1152, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), scale=0.28222981095314026, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.7625203132629395, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=8.392208099365234, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), scale=31.959184646606445, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=1.0, zero_point=0\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): QuantizableMBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), scale=176.03567504882812, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), scale=12.275124549865723, zero_point=128, padding=(1, 1), groups=1920, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): QuantizedHardswish()\n",
       "              )\n",
       "              (2): QuantizableSqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): QuantizedConv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1), scale=2.989643096923828, zero_point=128)\n",
       "                (fc2): QuantizedConv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1), scale=4.718342304229736, zero_point=128)\n",
       "                (activation): QuantizedHardswish()\n",
       "                (scale_activation): Hardsigmoid()\n",
       "                (skip_mul): QFunctional(\n",
       "                  scale=3.9211790561676025, zero_point=128\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): QuantizedConv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), scale=4.366452693939209, zero_point=128, bias=False)\n",
       "                (1): QuantizedBatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "            (f_add): QFunctional(\n",
       "              scale=85.03484344482422, zero_point=128\n",
       "              (activation_post_process): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Conv2dNormActivation(\n",
       "          (0): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=116.06055450439453, zero_point=128, bias=False)\n",
       "          (1): QuantizedBatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): QuantizedHardswish()\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (classifier): Sequential(\n",
       "        (0): QuantizedDropout(p=0.2, inplace=True)\n",
       "        (1): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): G0: (0.000274, 0.812364), G1: (0.000004, 0.780600), G2: (0.000029, 0.798829)\n",
       "  (2): QuantWrapper(\n",
       "    (quant): Quantize(scale=tensor([0.9168]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (dequant): DeQuantize()\n",
       "    (module): QuantizedLinear(in_features=1280, out_features=1, scale=0.04281321167945862, zero_point=128, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel.qconfig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harvard_gf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
